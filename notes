## To take some specific columns.
df[['col1','col2']]
1. Generate the random boolean list.
np.random.uniform(0,1,size=6) < 0.8
2. Generate the random list having values between [0,1].
np.random.uniform(0,1,size=6)
3. Count the number of NaN values in all the columns.
x.isnull().sum()
4. fill the null values with the average value.
x['Age'].fillna(x['Age'].mean(),inplace=True)
5. to find if list of values is in that column.
x[x['Embarked'].isin(['Q','S','C'])]
6. find unique values of that column
set(x['Embarked'])
7. Type of sklearn model.
KNN,Linear Regression,Ridge,Lasso,RandomForestClassifier,LogisticRegression,SVM
8. other classification and regression models are.
XGboost,lgboost,catboost,NN,rgf,Fm using keras
9. Hyper parameter tuning
GridSearchCV,hyperopt,Randomizedcv,bayesian optimization.
## various model methods for fit,predict,score
knn.fit(train_features,train_result)
prediction=knn.predict(test_features)
knn.score(test_features,test_result)
## metrics for prediction result.
accuracy_score(prediction,actual)
Mcc(prediction,actual)
log-loss
confusion_matrix(test_result,prediction)
grid={'n_neighbors':np.arange(1,20)}
knn_cv=grid_search.GridSearchCV(knn,grid,cv=3)
print(knn_cv.best_params_)
print(knn_cv.best_score_)
## to convert the array into one-hot encoding array.
np_utils.to_categorical(y_train, 10)
## axis=0 for rows and axis=1 for columns.
##for preprocessing and data splitting.
from sklearn.cross_validation import train_test_split
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit_transform(y)
##for getting the rank of the numpy array.
from scipy.stats import rankdata
aa=np.array([6,3,5,1])
rankdata(aa)
[4,2,3,1]
rankdata(aa,method='ordinal')	-assign consecutive ranks for the ties.
rankdata(aa,method='min')	-assign the minimum rank value to the ties.
##create the bin and specify the values into that.
bin = [0,3,6,9]
pd.cut('age',bin)
##for rolling the 2d array horizontally and vertically.
x = np.arange(10).reshape(2,5)
np.roll(x,1,axis=0)	-will roll the 2nd row to first and first to second.
np.roll(x,1,axis=1) -will roll the 1st column with last and last with first.

##seaborn and pandas for visualization
g=sns.FacetGrid(data=yelp,col='stars')
g.map(plt.hist,'text length',bins=50)

##for getting the correlation between the differenet columns.
stars = yelp.groupby('stars').mean()
stars.corr()
##heatmap to visualize the correlation relation.

##generate the random array of size 5*5 with mean 0 and standard deviation 0.1
np.random.normal(0,0.1,(5,5))
##To seed the value so that same random value got every time
np.random.seed(0)
##Activation functions sigmoid,tanh,Relu(Sparcity and Vanishing gradients),PRelu,LeakyRelu
sigmoid_func=1/(1+np.exp(-z))
derivative_sigmoid=sigmoid_func*(1-sigmoid_func)	## x*(1-x) will vanish the gradient when network is deep.
tanh=2*sigmoid_func(2z)-1
Relu=max(0,z)
0	;x<0
x	;x>0
derivative_Relu:
0	; x<0
1	; x>0

28*28-->using 5*5 kernel size and 3 different kernel-->3*24*24 units-->2*2 maxpooling--> 3*12*12 --> fully connected layer -->10 last units
O=(28-5+2*0)/1 + 1

O=(W-K+2P)/S + 1
O-output weight,W-input weight,K-filter size,P-padding,S-stride

border_mode -{'full','same','valid'}
input size-4*4
full-6*6
same-4*4
valid-2*2

--Reinforcement Learning--
state-action-reward: think that as a map with key as state with value as one list and one map.
list contain other state from current state with there probabilities.
second map contain available action for that state as key and reward as there value.

Q-learning- Learn the value of being in given state,taking specific action there.
Policy gradient(policy network)- Learn function which map an observation to an action.

Q-Learning is a table of values for every state(row) and action(column) possible in environment.
within each cell of table,learn a value for how good it is to take a given action within given state.
have 16 state and 4 possible actions.giving 16*4 table of Q-values.
update Q-table using Bellman equation.expected long-term reward for a given action equal to immediate reward from current action combined with expected reward from best future action.
Q(s,a) = r + gamma(max(Q(s`,a`)))
Q-value for a given state(s) and action(a) should represent current reward(r) plus maximum discounted (gamma) future reward expected according to our own table for next state(s')

import gym
gym.make('Cartpole-v0')
##will gave you initial(start) state of the env.
obser=env.reset()
##to render data directly to ui game
env.render()
##get the sample steps for the game from package
env.action_space.sample()
##to gave the agent action to perform
env.step(env.action_space.sample())

0  1  2   3
4  5  6   7
8  9  10  11
12 13 14  15

Types of variables-
1. Discrete- divided into nominal(categorical) like (gender,number of siblings) and ordinal
2. Continuous- which can take any value between minimum and maximum range. like (age,date and time). date and time can be separated into following new features(date,month,year,hour,minute,seconds) and these are further binning into groups like quatirily,half yearly etc.
Binning(for Continuous variable)- refer to dividing the variable into groups or bins.bins are easy to analyze but there is loss of information while creating the bins.
normaliztion using z-score((x-mean)/std) will help to find relationship in data more easily.
if data is left skewed in the graph then taking the log transformation will remove the skewness from the data distribution.

exploration-vs-exploitation
1. Epsilon-greedy strategy,where epsilon is the percent of time that agent takes a randomly selected action rather than taking the action that is most likely to maximize reward.
2. started with epsilon-(20%) for lot of exploration and then (10%) for doing exploitation
Markov Decision process-
1. Finite set of states - possible positions of mouse within the maze.
2. Set of actions available in each state- {forward,back} in a corridor and {forward,back,left,right} at crossroads.
3. Transitions between states- set of probabilities that link to more than one possible state.
4. Rewards associated with each transition- scalar reward are associated with each transition.
5. Discount factor gammma between 0 and 1- gave the difference between immedate rewards and future rewards.gamma is .9,then reward is .9^3*5.
6. Memorylessness- once current state is known,history of mouse travels can be erased because current markov state contains all useful info from history.

A policy function is a direct map from each state to the best coresponding action at that state.
Q learning attempts to learn the value of being in a given state and taking specific action there.

bellman equation get the current reward and then (gamma) will control the future reward.
value iteration vs policy iteration
value iteration is simple but computational heavy.finding optimal value function + one policy extraction.
whereas policy iteration is complicated and cheap than value.policy evaluation + policy improvement.

-----
Autoencoders-
it used encoder and decoder both. first encoder encoded the initial dataset(initial dimension) into some lower dimension then decoded back to the initial or some other dimension.it is the family of neural networks in which output is same as input.
Denoisisng Auto Encoder- Dae used for removing the noise from the dataset.By first adding the noise to the data(noise can't be random for categorical data beacuse there is different pattern in each column and row so can't add same noise to all columns.so take the related 15% data from the same row add to the existing). this way of converting the input data to low or high dimension vectors and reduce it back to same dimension as original. the loss is calculated by the difference in the output layer output and the original data.no need to do any feature selection or handling missing data.
data denoising(to remove noise) and dimensionality reduction for data visualization.
LDA, PCA and T-sne used for dimensionality reduction. In PCA(principal component analysis) used to get the longest region and find the plane on that and project the points on that plane.
those points are perpendicular to the plane. the plane can be found using eigen vectors.

--Recurrent, LSTM, Gru and Bidirectional--

LSTM-
get all unique characters from the text and vectorized that list
Bidirectional LSTM- in this the network has previous and after the word vectors.unlike in unidirectional LStm where only past words are available for the processing.
It involve the duplication of the first reccurent layer in the network so that there are two layers side-by-side,then providing the input to the first layer and reversed copy of the input sequence to the second.
GRU-idea of splitting the state neuron into different gates- input gate,output gate and forget gate.
can use the Textcnn in the Nlp task also.use the conv2d and maxpool2d after the embedding layer.try to use Elu and LeakyRelu as activation.
use CudNNLSTM,cudNNGRU layer for working the model in GPU.parameter for these layer are different from the normal layers.

CountVectorizer vs tfidfvectorizer-
countvectorizer will gave same weightage to all the words.it is equal to 1 if present else 0.
tfidf gave how important the word is in that doc.
Term frequency(Tf)- it will give you the how many times a particular word appears in the doc.like "the" come 100 times but it not gave us context about doc.
but if "messi" came 5 times it gave context like the doc belongs to sports.
you can take reciprocal of the word but it gave more weightage to the very rare word in the doc.
Inverse document frequency(Idf)- by taking the log(of number of docs in the corpus divided by number of docs in which that word appears.)
so this will gave 0 to the "the" and 0.8 to the "messi" and 1 to the rare word.
Tf Idf- we will multiple these two terms like "the" 100*0, "messi" 0.8*5=4, "rare word" 1*1=1.

Phrases- From individual words can make phrases.use bigram which will combine two words like(ice_cream,motor_cycle). can apply the bigram again to get the tigram(triplet words).

CountVectorizer for converting the text collection into matrix of token count.where each row is a unique word and each column is the review.
so it gave you sparse matrix which is mostly zeros and some one.so you will get the tupples like (row,column,value).
result will get the count of word occur with column number.
pad each sentence to the maximum sentence length.otherwise append special token to the sentence to make it's length to 59.
will create the vocabolary index and map each word to an integer 0 and 18,000.then each sentence became vector of integer.

Tokenization- process of converting text into tokens.and then remove the stopwords from the text which help to reduce the noise.
Stop words- words which can be removed from the sentence and still context wil not change.like(a,an,the).
Stemming- stripping the suffixes to get the root words. like-PorterStemmer()
Lemmatization- will use the vocabolary(dictionary importance of words.) and word structure and grammatical relations. like-WordNetLemmatizer()
Dependency Grammer and part of speech Tagging(POS)- it will map the words in sentence as nouns,pronoun,adverb,adjective.
Named Entity Recognization- finding the entities like(name,city,date,time,things etc.) from the sentence.
Sentiment analysis- distinguish between negative,positive and neutral experience.
language identification-can use the google or bing translation api or used the textblob package to do translation.
Text summarization- process of shortening the text by identifying main concept.

Word embedding-
these are just the vectors that represent multiple feature of a word.

Word embedding are the text converted into numbers.can be different 
1. create vocabulary with unique words
2. convert the word to one-hot vectors.where all the values are set to zero except for one.
3. word embedding create better vector representation of words.will create 10,000 * 300 sized matrix,where we look up a 300 length vector for each word.
4. will divide the whole sentence into group of words(of 5) with window size 5.
5. output probabilities going to relate to how likely it is find each vocab word nearby our input word.
6. learn statistics from number of times each pairing shows up.
7. create vocab of 10,000 unique words. going to represent an input words like "ants" as a one-hot vector.this have 10,000 components. will place "1" in the position corresponding to word "ants" and "0" on other places.

can use the glove pretrained vectors and apply in the embedding layer trainable=False so that weights of the embedding layers can't change.
for multilabel classification task use the sigmoid activation function and the loss equal to 'binary-crossentropy' instead of 'categorical-crossentropy' and softmax.
embedding layer first embed the sentence into vectors and then find the fixed latent space for each individual words.which are maps to the vectors.

take simple 3 layer network and pass the words to first layer then remove the output layer and take the hidden layer weight's which is the vectors for that word.
window size for the network is giving us back the 5 words before and 5 words after.network will learn the number of times each pairing shows up.
word2vec has two implementation CBOW and Skip-gram
CBOW- Wi-2,Wi-1,Wi+1,Wi+2 are the input to the model and it gave Wi as output."giving context it predict the word."
Skip-gram- input is Wi and output is Wi-2,Wi-1,Wi+1,Wi+2."predicting the context giving the word."
BOW- bag of words model example-
sent1 - "Hi fred how was the pizza?"
sent2 - "Hi fred, Pizza offer: two for one"
this will create the vocabolary like-
{hi,fred,how,was,the,pizza,offer,two,for,one}
gave the vectors like-
sent1 - [1,1,1,1,1,1,0,0,0,0]
sent2 - [1,1,0,0,0,1,1,1,1,1]
now can calculate similarity between two vectors by calculating the euclidian distance.
Continuous Bag of words:- 3-gram {'Hi fred how','fred how was','how was the','was the pizzza',..}
Skip gram-1 skip 3 gram:- {'Hi fred how','Hi fred was','fred how was','fred how the'}	there 'Hi fred was' skip over 'how'.so the skip gram is 1-skip 3-gram
Co-occurence:- pair of words say w1 and w2 is the number of times they have appeared together in context window.

libraries-gensim,fasttext,glove,nltk,spacy,gettext,TextBlob
TextBlob- work on top of Nltk library.can be used for the laguage translation.
spacy- is for part of speech tagging and named entity recognization
gettext- for language translation.
1. text preprocessing step.creation and usage of bag of words technique.
Stemming- applied to single word to derive its root.stemmer will convert 'walking','walked','walker' to its root word 'walk'.
Tokenization- breaking up the text into words.like 'Python is great' to token list ['Python','is','great'].
Bag of Words- for creating the unique list of words.
[“Star”, “Wars”, “Trek”, “better”, “good”, “isn’t”, “is”, “as”]
“Wars is good” to vector like [0,1,0,0,1,0,1,0].
n-grams of character- sunny is composed of[sun,sunn,sunny],[sunny,unny,nny]. so, it can find the word vector representation for the unknown or unique words.
models in fasttext-Skipgram and CBOW other than word2vec model in gensim library.
different type of pre-trained embedding-
Glove- having 50,100,300d vectors and (2M vocab words) for each words trained on wiki- corpus
Fasttext- having 2M vocab words with n-gram from (3-6) for misspelled words.
Word2Vec-

--Stacking ,ensembling and bagging--
bagging-
when building a forest,you don't select best trees to make similar classification.get diversity by training each tree on different data called bagging.they boost forest by training trees on hardest cases those that current forest gets wrong.helps to decrease the variance by averaging the number of models prediction.

1. main cause of difference in actual and predicted values are noise,variance and bias.ensemble helps to reduce that.
2. ensemble is the mean of all predictions to give final prediction.try to combine different models by arithmetic,geo mean or used other model to combine them.
Bagging- we build many independent model/learners and combine them using(weighted average,majority vote)
3. reduce the error by reducing variance.e.g.-random forest
Boosting- in which predictors are not made independently but sequentially.trees are grown using the information from previously grown trees.
when random subset of dataset is drawn then it is called Pasting.if sample drawn with replacement then called Bagging.
BaggingClassifier- 

Stratified K-folds- take the whole training dataset and split the set into k parts and then take the k-1 part and train on that then test on the part left.StratifiedKfold will return the indices of the numpy array.
which you can use to split your data.
first split data into k-1 parts then train on that and test on part left. use that left part for creating the train_validation(oof) part which could be used in second layer for stacking.
predict on the test data and take all the k-fold test prediction and average on that or you can do rank_mean and geometric mean on those fold test set and gave the more preference to the highest accuracy fold.

1. Add different columns with each other and use the result as the new column as extra features for creating models.can do this by adding single-single columns or adding multiple columns.can also (+,*,-,/) with the existing columns to create new columns.
2. can use numerical features like min,max,mean,std,var. etc.
3. Have to normalize the data before applying PCA.This way the variance in the data after applying pca is more(that means it doesn't lost too much info.).you can linearly combine the columns by applying pca.

1. GridSearchCv and RandomizedSearchCv are used for the hyperparameter tuning where gridsearch is more computationaaly expensive and search all the parameters while randomized search will randomly search the arguments. bayesian optimization is also a option.have to pass the dictionary of parameter and list of fields to search.
extratree parameter to tune is n_estimators,max_features,min_samples_split
xgboost param to tune are n_estimator,subsample,num_of_childs,num_trees.
lightgbm param to tune are n_estimator,subsample,num_of_childs,num_trees.

--Performance matrix--
Confusion Matrix- A table showing correct predictions and type of incorrect prediction made.
Precision- measure of classifier exactness.(Positive Predictive Value).if you get a list of 1 person and that is honest then you will have 100% precision but minimum recall. Number of true positive divided by number of true positive plus number of false positive.
Recall- measure of classifier completeness.if there are 100 honest person out of 300 how many you able to identify correctly.
True Negative- you classified not belonging to your class correctly.
False Negative- which you classified as not belonging to your class incorrectly.
True Positive- which you classified as belonging to the class correctly.
False Positive- which you classified as belonging to your class incorrectly.
F1 Score- weighted average of Precision and Recall. 2*(precision*Recall)/(precision+recall)
Kappa and Roc Curve,MCC(mathew correlation coeficient)
r2_score is for regression and roc_auc,MCC & f1 score is for the classification

penalized model- penalized svm etc. which would penalized the model for pridicting minority class wrongly.use Ridge and Lasso Regression for the penalization.
Ridge perform L2 regularization,add penality equal to square of the magnitude of the coeffeicient.
Lasso perform L1 reglarization,add the penality equal to the absolute value of the magnitude.

--Unbalanced Dataset--
Synthetic sampling with SMOTE(Synthetic minority over-sampling technique)-
1. first it will ignore the majority class examples.
2. for every minority instance choose its k neighbor.
3. create new instance halfway between the first instance and its neighbor.
4. not very effective for high dimensional data.
it will only generate examples within the body of available examples never outside.it can't create new exterior regions of minority class.
SMOTE- will take the feature vector and its nearest neighbour,compute distance between them.that diff multiplied by random number between (0,1) and added back to the features.
ADASYN- uses the density distribution to automatically decide the no. of synthetic sampls that must be generated for each minority sample.
BORDER SMOTE- it will create the minority and majority sample along the borderline of the distribution. which help to separate out minority and majority classes.
package-imblearn:-
under sampling techniques-
1. Cluster centroids
2. Condensed Nearest neighbor
3. Edited nearest neighbour/Repeated Edited nearest neighbour
4. Random under sampler
Over sampling techniques-
1. ADASYN
2. Random over sampler
3. SMOTE

underSampling- can take the majority class and then get the same number of minority class.
data_indices=data[data.healthy==1].index
random_indices=np.random.choice(data_indices,2,replace=False)
data.loc[random_indices]
-----

TruncatedSVD(singular value decomposition) is the sckit-learn implementation. which worked as PCA but can worked on large sparce-matrix.it will not centered the data before computing.

1. sequence_length- we padded all our sentences to have same length.
2. num_classes- number of classes in the output layer.
3. vocab_size- size of our vocabulary.
4. embedding_size- dimensionality of our embeddings.
5. shape of our embedding layer [vocab_size,embedding_size].
6. filter sizes- number of words we want our convolution filters to cover.     [3,4,5] means we have filters that slide over 3,4,5 words respectively.
7. num_filters- number of filters per filter size.
8. random seed- try to set the seed for getting the predictable result.

embedding layer contain a 3-d tensor of shape[None,sequence_length,embedding_size].
conv2d operation expect a 4-d tensor coresponding to batch,width,height and channel.add the channel manualy and layer became [None,seq_length,embedding_size,1]
create layer for each of different size filter's then get result from them and concat in one big feature vector.
each filter slides over whole embedding, but varies in how many words it covers
"VALID" padding mean that we slide filter over sentence without padding the edges.

Ridge - is for L2-Regularization
Lasso - is for L1-Regularization
ElasticNet- is Ratio of L1 and L2 Regularization

loss(theta) = basic_loss(theta) + k * magnitude(theta)
elasticnet
Regularization means applying downward on both number of mistakes we make as the magnitude of theta. k hyperparameter which modulate the tradeoff of how much downward pressure apply to the error of the classifier defined by theta versus magnitude of theta.

L2 regularization spreads error throughout vector x,so values of all the magnitude are small while some values are exactly zero while other may be large. L2 regularizer can unflatten the flat regions and curve up some stationary points without changing the minimum locations.
it is a way of convexifying a non-convex cost function to help gradient descent avoid some undesirable stationary points of function.

can be used in the feature selection from sparse modelling where only a small number actually contribute to the target variable you are trying to predict.like we try to predict cholesterol level or cancer by using more than 20,000 genetic features across human genome.where vast majority of genes have little or no effect on the presence or severity of most diseases.
Ridge Regression adds 'squared magnitude' of coefficient as penalty term for loss function while Lasso take absolute value of magnitude.Lasso shrinks the less important feature's coefficient to zero.

Correlation formula for Pearson Test:-
r = n(E xy) - (E x)(E y)/sqrt([nE x^2 - (E x)^2][nE y^2 - (E y)^2])

for models to stacks, first select the best accuracy models and different models then check correlation between them if pearson correlation value is less than 0.9 then these models are different and you can stack them.

Attention and Capsule layers:-
1. Attention in which you focused on one thing and see that in high resolution and surrounding you see in 'low resolution'.
2. Attention is simply a vector,often outputs of dense layer using softmax.
before attention,translation relies on reading a complete sentence and compress all info into fixed-length vector.so, a sentence with hundreds of words represented by several words will lead to info loss.
the length of outputs and inputs can be different.Vanilla RNN can handle fixed-length problem which is difficult for alignment.
3. It plug a context vector into gap between encoder and decoder.context vector take all cells output as input to compute the probability distribution of source language words for each single word.so,it can capture global information rather than one hidden state.
4. during decoding,context vectors are computed for every output word.so,have 2D matrix whose size is # of target words * # of source words.

Capsule - relationship between simpler features that make up a higher level feature.CNN solve this using max pooling or successive convolution layers.to reduce spacial size of data.need to preserve hierarchical pose relationalships between object parts.so that image classification will not depend upon the view angle.Capsule try to change the max pooling which loose the local relationship between the object parts.loose the precise location of the object in the image.
translational Equivariance - similar to invariance but it also predict where the object is (left or right) side in the image.maxpool is giving similar weight to whole kernel,so if image move slighty from center.

Byte Pair encoding- form of data compression in which most common pair of consecutive bytes of data replaced with byte that doesn't occur within data.
try to use the min-max scaling before finding the correlation.
callbacks for keras are:
EarlyStopping- wil stop if there is no improvement in loss for more than 2 epochs.
RocAucEvaluation,cyclicLR

Nesterov accelerated gradient(NAG)- start to slow down earlier.do big jump based on our stored information.
Adagard- adaptive gradient allows learning rate to adapt based upon parameters.perform larger update for infrequent parameters and smaller for frequent parameters.well suited for sparse data(like NLP or image recognization). in this learning rate is monotonically decreasing.caused the problem of learning stopping.learning rate calculated as one divided by the sum of sqaure roots,at each stage you add another square root to the sum which caused sum to constantly decreased.
AdaDelta,RMSProp- used sliding window over the past square root instead of taking sum of all.

Pytorch- framework similar to tensorflow,Theano,caffe. but it allow dynamic graphs instead of static graphs which is in tensorflow so,you can run each command one by one instead of writing all then run.Have pythonic syntax and (nn) module inside pytorch is the alternative to the keras API.
Tensor is the multidimensional array only.you can create different tensors and do mathematical operation on that.it's syntax is very similar to the numpy syntax.
Modules are in pytorch-
Autograd,optim,nn
Autograd(automatic gradient)- it has a recoder which record all the operation done and replay the operations backward for calculation of the gradients.
optim- this module provide various optimization methods like (Adam,SGD)
nn- provide keras like Api to build neural networks.

Pre-trained models-resnet50,resnet152,inception,vgg16,vgg19,inceptionresnetV2,mobilenet. can train only the last two layers of these model and used that for your own dataset.
or just removed the last output layer and add your own output layer. use the pretrained weights for those layer. use the low learning rate and flow_from_directoy keras method to load images from the directory folder one by one and these folder have same name as output label.image resizing can done using the same method.image augmentation and shifting of image to left or right can be done to increase the accuracy.

winner solutions-
1. Different Embedding - Have to use different type of embeddings for the model diversity.using same type of embedding leads to correlated models. using fasttext,glove for common-crawl,wikipedia and twitter,
BPEmb embedding solve the words not in vocabulary problem. this is the subword embedding which try to solve the unknown word problem.
LexVec is one more word embedding created using the wikipedia corpus.
2. pseudo_labelling - if accuracy is high near to 98 to 99 percent then use the pseudo-labelling technique and use the test prediction from the top model and use those as the training set.
3. data augmentation - where we have to create the artificial data from the existing data.
first split the training data into train and validation set and then do the data augmentation on the training set and used the validation data to get the idea that your model will work well on augmented data or not.
4. translation train time augmentation - translate the english comment to french,german or latin then again translate back to english by this way model will learn the noise in the data.
translate the english training data to some other language like (french,german,spanish) then used word embedding for those language for training.
5. concating Embedding - try multiple embedding (glove,fasttext) in one single model like (image RGB channels).concating two embedding together improve the score.
see which word are not in fasttext then find those words in glove and add it to the fasttext embedding file.
6. try to ensemble different type of models in the first layer of stacking try to use subset of models in all the ensembles. then in the 2nd layer of stacking average those first layer output or use different model on that.
7. try to combine different embeddings to make the 300d embedding to 900d like combined(Fasttext,glove,word2vec) vectors.
8. try to use the bayesian optimization to find hyperparameters of the model.
9. try to reverse the sentence order means last sentence come first and first sentence to the last.will preserve the word ordering within sentence but gave new neighbouring words where sentence joined.
10. do minmax scaling before calculating correlation between models.
11. can used high epochs value like (10,15) if we are using the CNN model.

binary classification task:-
1. features are (sum of values of rows,count of 0/1 in a row,max of rows,position of max in the row). if there are (yes/no,true/false) values are there then count of those val in a row.
2. add metafeatures in the second level of stacking.
3. Boruta feature selection for finding the important features in the datset.also get the rank for the various features
4. Denoising autoencoder(Dae) for feature selection and removing the noise from the dataset.train the denoising autoencoder for 150 epochs and try to get loss of(0.008)
5. One hot encoding will increase the score.label encoding for categorical data(like gender etc.) and feature interaction like(+,-,*,/) can be used between different features.
6. you can create the multiclass,binary or regression model and then try to ensemble them.


--Chrome extension--
1. manifest.json is the main file in the chrome extension this will be loaded first by the chrome it contain all the info including the javascript file to be loaded for that.
2. "js": here you can add the avascript file to be loaded.
3. "matches": this will gave you the link to which we have to match the link,for which you want to run that extension.
4. in "js" field add the js file in order which you want to load.
5. content script can't access all the API (like can't listen for clicks on the browser action),so need to add a diff type of script called background script which has access to every Chrome API,but can't acces the current page. so add the "background" tag in the manifest.json for the loading of background script.
6. so your chrome extension file have following files.
background.js
content.js
icon.png
jquery.min.js
manifest.json
